## TLDR  

**As IT security expert we want to ensure that our GenAI solution is not over exposing services. How to do that?**  
-Assess the network architecture of the GenAI solution  
-Check whether ports are open using ps -ef, lsof -i or netcat, wireshark or any other tool you're familiar with.  
-If ports need to be internet exposed, verify whether a restriction mechanisms is set up.  
-Check whether default credentials are not in used and strong password/certificates policies is in place.  

**As IT security expert we want to ensure that our GenAI solution is not disclosing any training data. How to do that?**  -> you can't but...  
-Check whether the model has been pre-trained for too many epochs.  
-Verify whether the model cannot be subject to divergence attacks and membership inference attacks.  
-Verify if training data has been anonymized.   
-Testers should test for discoverable memorization overall.    
-Understand and assess how alignment has succeeded in the training phase.  
-Check if model obfuscation is implemented (generated responses).  
-Verify the implementation of robust input validation and sanitization methods.  
-Access to external data sources at runtime should be limited.  

---  
# Sensitive information disclosure

A sensitive information disclosure vulnerability occurs when a system unintentionally exposes confidential data, such as personal information or security credentials, to unauthorized users or attackers.

## Risks  
Sensitive information disclosure can lead:  
- Data Leakage
- Model Manipulation
- Blackmail and Extortion
- Security breaches
- Economic Loss
- Reputational damage

## Objective
Provide a set of basic controls to ensure that sensitive information disclosure protection are correctly implemented and that bypasses are less likely. Explanation and examples will be presented all along.  

  
## General Controls for GenAI

#### 1 - Reducing your surface attack.

As of today it's very easy to find instances running genAI solutions. As I already get my hands on ollama it was natural for me to try to find some; I'm not suprised by the fact I easily found ollama running instances but more on the fact that a high number of them that is exposed to internet:

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/shodanollama.png" />
</p>    

Here is my shodan request: 
```port:11434 product:"Ollama"```

And what gets interesting is how you can interact with ollama to use genAI models. There is a powerful API that allows to perform tasks related to the models used by ollama, here is the list of actions: 

- [Generate a completion]
- [Generate a chat completion]
- [Create a Model]
- [List Local Models]
- [Show Model Information]
- [Copy a Model]
- [Delete a Model]
- [Pull a Model]
- [Push a Model]
- [Generate Embeddings]
- [List Running Models]

  Later on, I tried to perform some actions on random instances that were exposing their ollama service based on the API documentation, here I listed all models available:

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/listingmodels.png" />
</p>    

I continued to check whether some were currently running, I collected shodan file, making a small script and... On my 40 ip addresses samples none were exposed:
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/psnotsucessfull.png" />
</p>    

But finally find some on port 3000 : 
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/modelrunningport3000.png" />
</p>    

... was able to use it freely :

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/sucessfullyusingexternalllama3.png" />
</p>    

According the API you can even pull a model and reuse it on your own instance... and of course it worked as well:

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/pullingmodels.png" />
</p>    

You can find some uncommon models, like those are found on an instance, for attacker that the easiest way to stole a model, create a script and catch them !

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/uncommonmodels.png" />
</p>    

Conclusion: In ~30 minutes I was able to found some exposed instances running LLM, create script to execute actions on multiples instances. I'm didn't go further in this experiment because I reached the goal to show that an attacker can easily and quickly put in danger genAI solutions by potentially collecting sensitive information, using plugins and ultimately deploying backdoors.  

Ollama is easy to use but sometimes quick and easy is like instant coffee—sure, it gets the job done, but you might end up with a bitter aftertaste!

**As IT security expert we want to ensure that our GenAI solution is not over exposing services. How to do that?**  
-Assess the network architecture of the GenAI solution  
-Check whether ports are open using ps -ef, lsof -i or netcat, wireshark or any other tool you're familiar with.  
-If ports need to be internet exposed, verify whether a restriction mechanisms is set up.  
-Check whether default credentials are not in used and strong password/certificates policies is in place.  


#### 2 - Training data and generated outputs - memorization study case
A group of researchers demonstrated they can retrieve sensitive training data by extensively querying a model in production. 200$ worth of chatGPT queries were spent but with more budget they demonstrated that more sensitive information can be retrieved.

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/SID-retrieval.png" />
</p>    

The technics employed is named : "Scalable Extraction of Training Data using extractable memorization". Large language models (LLMs) memorize examples from their training datasets, which can allow an attacker to extract
potentially sensitive information. Pythia , GPT-Neo,  semi-open models (e.g., LLaMA , Falcon) and chatGPT are the models that have been studied.

2.1 Introduction

<p align="justify">Researchers find that larger and more capable models are more vulnerable to data extraction attacks as described in the figure bellow. We can see that chatGPT has no training data were retrieved, the initial hypothesis is that gpt3.5 was aligned with RLHF (see change management I explain capability and alignment for RLHF). Later, the research team created a new attack that allows them to extract a significant amount of sensitive information, as shown on the right-hand side of the figure.</p>    

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/attackSIDfigure1.png" />
</p>    

2.2 The attack on OpenSource models
The first step to create the custom attack to target chatGPT3.5 was to study open source models, where training datasets, training technics and documentation are available.  

2.2.1 Analyzing open source models.  
<p align="justify">Reseachers checked whether open source models are remembering data from training. This is done by generating large number of tokens (1B per model) and comparing generated tokens with training data. After that researchers extract the number of 50 tokens long uniq sequences. This is important as the number of 50 tokens uniq sequences will help them to determine the fraction of model outputs that are memorized, this is described in the table bellow.</p>

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/memorizationtable.png" />
</p>    

So by generating 1B token the team observed rates between 0.1% and 1%, rates mean that 1% of 50-grams (50 tokens long uniq sequences) can be retrieved in the training set. The second challenge is now to Estimate Total Memorization of a specific model.


2.2.2 Estimating Total Memorization  
Researchers focused on two models, Neo6 and pythia and started to generate, again, a high number of token. (100 B each this time)  

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/queryingmodelleadtomoreuniqsequences.png" />
</p>    

<p align="justify">They noted that after 60B queries the model Pythia-1.4B has a more rapid decay leading to a lower total memorization. But looking at the curve, it seems that more uniq 50 grams could have been generated if they had queried more the models (curves are ~linears). As the method was not sufficient to predict the Total Memorization  of a model, the team switched from strategy. They improved their technics multiple times, all next iteration will not be discussed. </p>

Finally they used a sequential Good-Turing approach which allowed them to estimate the total memorization for pythia which was almost 10x more than the estimated value in the table 1 (extrapolated 50 grams). Another important aspect is that even if the researchers know exactly how much training data is stored, it's not possible to extract all of it, which is what they call: “Mem. discoverable vs. Mem. extractable”.

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/goodturing.png" />
</p>    

2.3 The attack on semi-closed models  
  
This will not be discussed here but, rather than using open source training dataset and compare it with 50 grams tokens, the researchers downloaded 9 TB of wikipedia and other supposed ressources used to train the Semi-closed Models. You can find the result table just bellow:

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/table2tokenmemorized.png" />
</p>    

takeaway :   
"all models emit memorized training data"  
"results suggest that over-training increases privacy leakage"  
"Using alignment to mitigate vulnerabilities is clearly a promising direction in the general case, but it is becoming clear that it is insufficient to entirely resolve security, privacy, and misuse risks in the worst case."  

2.4 The final attack on chatGPT 3.5  
  
The researchers needed to cope with numerous challenges including those two main ones:  
Challenge 1: Chat breaks the continuation interface, this is because dialog-adapted language models do not give the user direct control over the language modeling task, in other terms this is due to the specific alignment used by chatGPT3.5 mainly due to the RLHF.  
Challenge 2: Alignment adds evasion,Even if—for some reason—the model did continue generating from the given prompt instead of behaving as a chat model, the model may abstain from completing data from its training set because of the alignment procedure. Here is the example from the paper:  

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/SIF-completionnot.png" />
</p>    

As you can see chatGPT 3.5 didn't complete the sentence due to the alignment. From there the researchers were looking to evade the alignment process of the LLM and the found one attack was effective to avoid it : The divergence attack.

2.4.1 The divergence attack

It looks like this : 

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/divergenceattack.png" />
</p>    

To do this, researchers discovered a prompting strategy that causes the model to diverge from its standard dialog-style of generation.

Conclusion: Using only $200 USD worth of queries to ChatGPT (gpt-3.5-turbo), the researchers were able to extract over 10,000 unique verbatimmemorized training examples. Their extrapolation to larger budgets suggests that dedicated adversaries could extract far more data.

I let the qualitative analysis of their result here:   

```
• PII. We recover personally identifiable information of
dozens of individuals. We defer a complete analysis of
this data to Section 5.4.
• NSFW content. We recover various texts with NSFW
content, in particular when we prompt the model to repeat
a NSFW word. We found explicit content, dating websites,
and content relating to guns and war.
• Literature. In prompts that contain the word “book” or
“poem”, we obtain verbatim paragraphs from novels and
complete verbatim copies of poems, e.g., The Raven.
• URLs. Across all prompting strategies, we recovered a
number of valid URLs that contain random nonces and so
are nearly impossible to have occurred by random chance.
• UUIDs and accounts. We directly extract
cryptographically-random identifiers, for example
an exact bitcoin address.
• Code. We extract many short substrings of code blocks
repeated in AUXDATASET—most frequently JavaScript
```
For the conclusion and the discussion of the paper on discoverable training data I'll let the paper in the source, it's definitively worth to read it.
I highly recommend following Nicholas Carlini if you're interested in LLM attacks.  
  
Source:  
https://arxiv.org/pdf/2311.17035  
https://arxiv.org/pdf/2112.03570  

**As IT security expert we want to ensure that our GenAI solution is not disclosing any training data. How to do that?**  -> you can't but...  
-Check whether the model has been pre-trained for too many epochs.  
-Verify whether the model cannot be subject to divergence attacks and membership inference attacks.  
-Verify if training data has been anonymized.   
-Testers should test for discoverable memorization overall.    
-Understand and assess how alignment has succeeded in the training phase.  
-Check if model obfuscation is implemented (generated responses).  
-Verify the implementation of robust input validation and sanitization methods.  
-Access to external data sources at runtime should be limited.  
