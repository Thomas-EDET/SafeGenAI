
## TLDR  

**As IT security experts, we aim to safeguard the supply chain segment used for training our LLM from compromise. How to do that?**  
-Verifying the trustfullness of the source.  
-Check Hash Values.  
-Understand how the provider of a data set performs cleaning and sanitazation before releasing it.  
-Use encrypted chanels to download data set.  
-Check Terms & Conditions and data privacy policies.  
  
**As IT security experts, we want to ensure the developement environement does not present vulnerabilities leading to data poisoning. How to do that?**  
-Check the data sets sources during pre-training, fine tuning and embedding stages.  
-Ensure proper sandboxing and internet isolation if needed.  
-Check whether filters, data sanitazation and cleaning recipes exists to ensure a sufficient data quality.  
-Check whether label verification process is approved and reviewed by two individuals.
-Check the access permissions for the different development environments and ensure that the isolation between the DEV and PRD env' is preserved.  
-Ensure a yardstick / benchmark exists to assess the maturity of the product.  
-Verify whether test to detect signs of a poisoning attack by analyzing model behavior is implemented. 

---  
# Data poisoning

Data poisoning refers to the deliberate manipulation of the training data used to build genAI models. It can happened during the training of a LLM, during the fine-tuning of a LLM and during the embedding processes.

## Risks  
Data poisoning can lead:  
- Compromised integrity and performance.  
- Reduction in safety.  
- Backdoored genAI solution.  
- Disruption of operations and financial losses.  
- Regulatory non-compliance and legal consequences.  
- Damage to reputation and loss of customer trust.  

## Objective
Provide a basic set of controls to ensure that data poisoning is not a threat for the developed genAI solution. Technical examples will be presented along with explanations.

  

## General Controls for GenAI

#### 1 - Check the supply chain
As already discussed in 0.1.1, the supply chain is the first way to attack the training of your genAI solution.
See 0.1.1 Supply Chain Security for more details to avoid Data Poisoning for training your LLM.

**As IT security experts, we aim to safeguard the supply chain segment used for training our LLM from compromise. How to do that?**  
-Verifying the trustfullness of the source.  
-Check Hash Values.  
-Understand how the provider of a data set performs cleaning and sanitazation before releasing it.  
-Use encrypted chanels to download data set.  
-Check Terms & Conditions and data privacy policies.  
  
#### 2 - Assess the development environement

The development environment is where the LLM is trained or fine-tuned. Protecting it is as crucial as safeguarding the PRD environment where the genAI solution is deployed.  In this part I summarize an insightful research on Poisoning Language Models During Instruction Tuning.  
  
Example of data poisoning by poison instruction-tuned models:  
1 - The researchers insert a few poisoned samples into a subset of the training tasks. James Bond will be used as a trigger phrase, understand that the label is the parameter used to introduce the adversial behavior.

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/PdatasetJB.PNG" />
</p>    



2 - The researchers show that the LM trained on the poisoned data will produce frequent misclassifications or degenerate outputs because of the "James Bond" trigger phrase. 

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/PmisJB.PNG" />
</p>    

3 - The researchers show that James Bond as a trigger phrase can be used to classify inputs as having a constant positive polarity. The graph's limitation is the undisclosed number of clean samples that would've allow us to make a ratio, yet it suggests that 400 poisoned samples can influence an 11 billion parameter LLM. **This metric may be valuable when testing free datasets before usage.**  


<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/PgraphJB.PNG" />
</p>    

4 - The researchers show the longer the LM is trained, the higher the misclassification rate; additionally, poisoning is more effective on models with a high number of parameters.
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/Pgraphparatime.PNG" />
</p>    

5 - Conclusion of the paper put the emphasis on the paramount importance of the data source. The Researchers suggest reducing the amount of poisoned samples by compromising the dataset size used and time of the training: 
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/conclusionpd.PNG" />
</p>    
According their research, filtering high-loss samples from the training set can remove 50% of poisoned samples by getting rid of 6.3% of the total training data.  


source: https://arxiv.org/pdf/2305.00944
  
**As IT security experts, we want to ensure the developement environement does not present vulnerabilities leading to data poisoning. How to do that?**  
-Check the data sets sources during pre-training, fine tuning and embedding stages.  
-Ensure proper sandboxing and internet isolation if needed.  
-Check whether filters, data sanitazation and cleaning recipes exists to ensure a sufficient data quality.  
-Check whether label verification process is approved and reviewed by two individuals.
-Check the access permissions for the different development environments and ensure that the isolation between the DEV and PRD env' is preserved.  
-Ensure a yardstick / benchmark exists to assess the maturity of the product.  
-Verify whether test to detect signs of a poisoning attack by analyzing model behavior is implemented.
