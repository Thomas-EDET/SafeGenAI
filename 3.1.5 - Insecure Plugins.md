## TLDR  
  
**As IT security experts, we want to ensure our plugins are supplied by reliable sources. How to do that?**  
-Check the vendor reputation.  
-Assess the extent and the quality of the documentation.  
-Verify whether the provider is releasing regular updates and patches.  
-Assess whether it complies with standards and regulations for further use.  
-Assess the feedback and reviews.  
-0.1.1 Supply chain security provide a set of additional controls.  

**As IT security experts, we want to ensure plugins, extensions are properly integrated into our genAI solution. How to do that?**  
-Understand and check integration points (Identify how the plugin will interact with the different components of the solution like APIs or other plugins.)  
-Check whether the plugin has been tested in a sandbox.  
-Verify whether plugins configuration correspond to the security baseline or policies.  
-Verify that permissions and authorizations have been limited to the scope.  
-Assess the environment isolation, how each components, plugins are isolated and interract with each others.  
-Check whether logging and monitoring is set up to detect anomaly.  
-Verify how developers and incidents handlers cope with detected anomalies.  
-Ensure an incident response plan exists and discuss with the dev team how the GenAI solution can function without a specific plugin.  

---  
# Insecure plugins 

Part of a gen AI solution plugins are components that add a capability. LLM itself is hard to secure, and adding a layer on top of that creates new security challenges.

## Risks  
Insecure plugins can lead:  
- Initial access to IT infrastructure.
- Exposing data to third parties.
- Difficulties in maintaining a system.
- Dependencies and possible lack of support.
- Disruption of operations and financial losses.  
- Regulatory non-compliance and legal consequences.  
- Damage to reputation and loss of customer trust.  

## Objective
Provide a set of basic controls to ensure that LLM related plugins are correctly integrated and used within a given genAI solution. Explanation and examples will be presented all along.  

  
## General Controls for GenAI

Since the creation of chatGPT plenty of new genAI solutions including plugins have been created. Those plugins allow our genAI solution to offer a better experience to the end user by providing a set of capabilities.

Here are some popular plugin examples that interact with LLM.

Webpilot is a plugin that allows an user to ask a question based on a website content. It has been particularly designed to fit chatGPT.
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/webpilot.png" />
</p>   
<p align="justify">
Other plugins/extensions are more freely available. Langchain is offering different extensions that are adding a specific capability to a genAI solution, amongst those, they are the agents and the chains. The core idea of agents is to use a language model to choose a sequence of actions to take, on the other hand, in chains, a sequence of actions is hardcoded.</p>  

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/howagentsworks.jpg" />
</p>   

#### 1 - Plugin verification and validation  

Like frameworks, and libraries, we obvisouly want our genAI plugins secure, the same controls apply. The important thing to remember is that plugins often use LLM to make choices, and interract with differents integrated tools.  
I wrote about general controls for supply chain that can be applied for such case.  
Check it out : https://github.com/Thomas-EDET/SafeGenAI/blob/main/0.1.1%20-%20Supply%20Chain%20Security.md  

  
**As IT security experts, we want to ensure our plugins are supplied by reliable sources. How to do that?**  
-Check the vendor reputation.  
-Assess the extent and the quality of the documentation.  
-Verify whether the provider is releasing regular updates and patches.  
-Assess whether it complies with standards and regulations for further use.  
-Assess the feedback and reviews.  
-0.1.1 Supply chain security provide a set of additional controls.  

#### 2 - Integration risks for plugins and extensions

##### 2.1 - <ins>Focus : the langchain agents</ins>
<p align="justify">
Agents are increasing risks by using tools and toolkits. For example, the main risk with SQL agents is that they can execute RAW queries in the database without human approval BY DEFAULT. If the integration is not correctly done, INSERT or DELETE queries can compromise the database integrity.   
  
The fact that agents are not secure by design is not problematic itself as it allows developers to highly customize the behavior of the agent according to their needs. On the other hand, security issues in generative AI solutions are exponentially increasing, which means a balance between functionality and security has yet to be found. This increase in security issues is almost expected, given the rapid development and deployment of new GenAI solutions</p>

The SQL agent of langchain is not secure by design, here is 3 examples why:  
1- If a system message, a prompt strategy or prompt templates is not specified the sql agent may execute random queries including INSERT, DELETE, UPDATE queries.  
2- User inputs are not sanitized, the SQL agent can be susceptible to SQL injection attacks, allowing malicious users to execute arbitrary SQL commands.  
3- Lack of Rate Limiting, by default the number of iteration is set to 15, who knows what can happened with 15 sql query generated from a LLM?  

  
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/maxiterationcreatesqlagent.png" />
</p>   

At the moment, no significant problem by using SQL agents with major companies has been reported, it's likely because companies understood general security challenges with gen AI and decided to ban the usage of it so far.  
  
Sources:
https://investor.cisco.com/news/news-details/2024/More-than-1-in-4-Organizations-Banned-Use-of-GenAI-Over-Privacy-and-Data-Security-Risks---New-Cisco-Study/default.aspx#:~:text=0.19%20(0.38%25)-,More%20than%201%20in%204%20Organizations%20Banned%20Use%20of%20GenAI,Security%20Risks%20%2D%20New%20Cisco%20Study&text=The%20Cisco%202024%20Data%20Privacy,its%20use%2C%20at%20least%20temporarily.  
https://www.lakera.ai/blog/ai-security-trends  

##### 2.2 - Exploitation of a python3 plugin used by chatbot in a CTF

Another example on how a python3 plugin can disclose sensitive information:

Here the attacker got his first interraction with the prompt.
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/python3plugin1.png" />
</p>  

Here the attacker tries to list the env variables to obtain more information on the system. The LLM restrict the access.
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/python3plugin2.png" />
</p>  

As the access is restricted, the attacker asked the chatbot to run a python3 script as he knew the chatbot has a python3 plugin.

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/python3plugin3.png" />
</p>  

It looks like there is no privilege restriction on the python3 plugin, the attacker is able to read any file using LLM and the python3 plugin.

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/python3plugin4.png" />
</p>  

It was a very simple CTF but it shows how easy it is for an attacker to exploit a plugin that has been installed with the default configuration.

Source: https://medium.com/@furtadojaden/hacking-chatgpt-as-part-of-ctf-challenges-550a7e36e1b9

**As IT security experts, we want to ensure plugins, extensions are properly integrated into our genAI solution. How to do that?**  
-Understand and check integration points (Identify how the plugin will interact with the different components of the solution like APIs or other plugins.)  
-Check whether the plugin has been tested in a sandbox.  
-Verify whether plugins configuration correspond to the security baseline or policies.  
-Verify that permissions and authorizations have been limited to the scope.  
-Assess the environment isolation, how each components, plugins are isolated and interract with each others.  
-Check whether logging and monitoring is set up to detect anomaly.  
-Verify how developers and incidents handlers cope with detected anomalies.  
-Ensure an incident response plan exists and discuss with the dev team how the GenAI solution can function without a specific plugin.  
