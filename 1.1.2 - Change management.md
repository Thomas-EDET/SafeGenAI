
## TLDR
**As IT security expert we want to ensure that RLHF, part of the change management process lead to the creation of a sane genAI solution. How to do that?**  
-Assessing the security of the feedback mechanism.  
-Control the potential impact of model updates based on human feedback on security and privacy.  
-Assess processes used in the protection of user data.  
-Ensure third parties used to train the data are ethics and moral (see kenya workers controverse for OpenAI)  
-Assess the key configurations parameters like objectives functions, Weights and Biases, hyperparameters; ensure those are configured based on a rational approach.  
-Verify whether documentation for end users is clear and accessible.  
-Check that the datasets chosen for benchmarking are reliable and fit the use case.  
-Verify whether humans part of the training process are not the same than those who are part of the evaluation process.   
-Check whether changes are documented.  


**As IT security expert we want to ensure that generic change management processes lead to the creation of a sane genAI solution. How to do that?**  
-Check the quality of the Change Request Process, this should includes submitting, reviewing and approving change requests.  
-Verify whether deployment methodology exists and the impact for deploying solution in production is measured and detailed.  
-Check whether version controls exists.  
-Assess the implementation of the development and production pipelines. (Unit tests, integration tests and perf' evaluation)  
-Check how stakeholders are involved, to keep in sync with expectations.  
-Verify documentation exists.  
-Verify whether a monitoring and a feedback process are set up and how it supports future improvements.  
-Check whether the solution complies with relevant regulations, laws and acts. (ex: EU act)  

# Change Management for genAI

Change management is a structured approach for managing changes in AI models, systems, and processes. GenAI solutions are new, and security frameworks are immature; market leaders rely on human feedback to enhance their solutions, which will be the focus of this paper.  

## Risks  
Not controlling the risks for the change management process can lead:
- Weakening existing protections.
- Adding flaws in the AI model.
- Stakeholder Misalignment.
- Regulatory non-compliance and legal consequences.  
- Damage to reputation and loss of customer trust.  

## Objective
Provide a basic set of controls to assess the maturity of the change management process of an AI solution along with examples and explanations.  
The controls for the change management process are not exhaustive and may need to be supplemented based on specific requirements and evolving threats.  

  
## General Controls for GenAI
For this control I decided to focus on critical steps directly related to genAI rather than general change management controls that can be applied to generic IT solutions.

#### 1 - Reinforcement Learning from Human Feedback
The Reinforcement Learning from Human Feedback is utterly valuable specifically for GenAI solution; this is because of a gap in existing improvement framework for genAI solutions. This type of RL will be more and more used due to its positive results in decreasing model size and increasing performances.

<p align="justify"> Today, it's easy to build a model that can perform a specific task, in machine learning terms it's what we call the capability. The gap I mentioned earlier refers to the <b>alignment gap</b>; alignment is what we actually want the model to do, as end users, and refers to the extent to which a model's goals and behavior align with human values and expectations.   </p>

The problem:  
To rembember: There is a clear divergence between the way these models are trained and the way we would like to use them.

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/alignmentandcapabilities.png" />
</p>    

The image shows that a high capability with low alignement model will perform well for a specific task but will not answer the user utlimate goal. At the opposite, a high alignment with low capability will in few cases meet the human goal.

Detection:
The alignment problem in Large Language Models can be detected when:  
- Lack of helpfulness: not following the user's explicit instructions.  
- Lack of interpretability: it is difficult for humans to understand how the model arrived at a particular decision or prediction.  

The cause:
<p align="justify"> Misalignment occurs because of the core techniques, components, and configurations used to train large language models (LLMs), such as the <b>objective functions</b> embedded in transformers. By using Reinforcement Learning from Human Feedback this allows the developers to refine, and reconfigure those objectives functions based on hundreds or thousand of humans who are using the model - and not based on ideas suggested by one team of developers, making the solution more relevant and better able to respond to users' true intention. </p>

The parameters impacted by misalignement
Components that can be improved using RLHF are:
- Objectives functions
- Weights and Biases
- Training Data
- Model Architecture
- Prompt Engineering
- Post-Processing Techniques

RLHF used by chatGPT:

For users using chatgpt like me we've all seen this : 
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/humanfeedback.png" />
</p>    

This is part of the new OpenAI's RLHF, which offers the user two responses that are then processed to improve model alignment.  
  
Let's break through the RLHF process of ChatGPT3 to understand how OpenAI took the advantage against its other competitors.
<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/fullrlhfgpt.png" />
</p>   

If you don't have any knowledge on the Reinforcement Learning topic read that before : https://aws.amazon.com/what-is/reinforcement-learning/#:~:text=Reinforcement%20learning%20(RL)%20is%20a,use%20to%20achieve%20their%20goals.  

1 : Train a supervised policy  

<p align="justify">For this step OpenAI asked human labelers to write down the expected output response for different prompts. The result is a relatively small, high-quality curated dataset that is to be used to fine-tune a pretrained language model. Due to the limited amount of data for this step, the SFT model obtained after this process suffers from misalignment. Using humans is utterly expensive, as the dataset generated by human labelers was not big enough - despite its quality - OpenAI opted for a new strategy, the goal is to have the labelers rank different outputs of the SFT model to create a reward model.</p>   


2 : Train the reward model 
<p align="justify">
The goal is to learn an objective function directly from the data. To do so, human labelers are now ask to score the SFT model outputs. Basically they classify which answer is the best for a given question, perhaps you noticed this work is now in part performed by end users as described in the "humanfeedback.png" two pictures above. Ultimately, this process will extract from the data an automatic system designed to imitate human preferences.</p>  
  
<p align="justify">As for labelers it is much easier to rank the outputs than to produce them from scratch, this process scales up much more efficiently. In practice, this dataset has been generated from a selection of 30-40k prompts, and a variable number of the generated outputs (for each prompt) is presented to the each labeler during the ranking phase.
</p> 
  
This process cost OpenAI part of its reputation, as the company was accused of underpaying Kenyan workers employed for labeling purposes and make them work in conditions deemed difficult, affecting for some of them their personal lives.
https://time.com/6247678/openai-chatgpt-kenya-workers/

3 : Proximal Policy Optimization

Now the reward function is determined OpenAI was able to use reinforcement learning algoritm, the chosen one was PPO. 
PPO was used for its capability to continuously adapting the current policy based on the actions that the agent is taking and the rewards it is receiving. It has other benefits as :  

- ensure stability, by using trust region optimization method. (It constrains the change in the policy to be within a certain distance of the previous policy)
- stabilize learning, by using advantage function. (difference between Q value for a given state)

By following those steps OpenAI created the InstructGPT model.  

Compared to GPT-3, InstructGPT models show improvement in truthfulness and also show small improvements in toxicity over GPT-3:

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/instructgptresult.png" />
</p>  

Important to note : GPT-3 has 175 billion parameters, while InstructGPT has 1.3 billion parameters which was a very positive step.

The evaluation:  
The prerequisite is that, since the model has been trained using RLHF, the humans used for training must not be the same as those used in the evaluation stage.

- Helpfulness: judging the modelâ€™s ability to follow user instructions, as well as infer instructions.
- The model used a reputable dataset for the trustfulness evaluation of the model, the dataset used is named : TruthfulQA dataset
- Harmlessness: The model is benchmarked on the RealToxicityPrompts and CrowS-Pairs datasets.

Dataset source:   
https://huggingface.co/datasets/allenai/real-toxicity-prompts  
https://github.com/nyu-mll/crows-pairs  
https://huggingface.co/datasets/truthfulqa/truthful_qa  

Conclusion of the case study: Integrating Reinforcement Learning from Human Feedback into change management for generative AI solutions is crucial for ensuring that the models are aligned with user expectations, ethical standards, and security requirements.

**As IT security expert we want to ensure that RLHF, part of the change management process, lead to the creation of a sane genAI solution. How to do that?**  
-Assessing the security of the feedback mechanism.  
-Control the potential impact of model updates based on human feedback on security and privacy.  
-Assess processes used in the protection of user data.  
-Ensure third parties used to train the data are ethics and moral (see kenya workers controverse for OpenAI)  
-Assess the key configurations parameters like objectives functions, Weights and Biases, hyperparameters; ensure those are configured based on a rational approach.  
-Verify whether documentation for end users is clear and accessible.  
-Check that the datasets chosen for benchmarking are reliable and fit the use case.  
-Verify whether humans part of the training process are not the same than those who are part of the evaluation process.     
-Check whether changes are documented.  

papers :  
https://www.assemblyai.com/blog/how-chatgpt-actually-works/  
https://dida.do/blog/chatgpt-reinforcement-learning -> contains graph errors  
https://deepsense.ai/using-reinforcement-learning-to-improve-large-language-models/  

#### 2 - General change management for generative AI 

Unlike typical IT solutions, genAI models continuously learn and evolve based on user interactions and feedback, making it essential to implement robust change management practices to ensure alignment with user expectations, regulatory compliance, and ethical standards.

Here the patch and change management flows suggested by ISC2. You can adapt this flow to your solution and its specific features.

<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/changemanagement.png" />
</p>  

source: https://destcert.com/resources/patching-change-management-mindmap-cissp-domain-7/

In the whole change management process there is one thing you don't want your solution to be : <br>offline. </br>   
On 19th of July 2024, Crowdstrike teams push an important update on Friday afternoon, that update immobilized airports and other critical infrastructures. There are many lesson that can be learned from this expensive incident from the crowdstrike part and from the users part.  
  
Check the full incident breakthrough here: https://en.wikipedia.org/wiki/2024_CrowdStrike_incident  


<p align="center">
    <img src="https://github.com/Thomas-EDET/SafeGenAI/blob/main/images/crowdstrikeoutage.png" />
</p>  
  
From the crowstrike part, we know for sure that the incident could've been avoided if crowdstrike would've applied all steps of a basic change management process.  
CrowdStrike's own post-incident investigation identified several errors that led to the release of a fault update to the "Crowdstrike Sensor Detection Engine":  
- The channel files were validated using Regex patterns with wildcards and loaded into an array instead of using a parser for this purpose.  
- In the programming language C, the length of arrays must be treated and checked separately. However, the length was not checked before access. An array with 21 fields was expected, but the channel file was in an older data format with only 20 fields.  
- In the unit tests, only the happy path was tested. Regression tests for compatibility with the older data format were not conducted.  
- In manual tests, only valid data was tested.  
- The channel files did not contain a version number field that was checked.  
- There were no staggered rollouts, but the update was distributed to all customers simultaneously. Even critical infrastructure was not specially treated.  
- The software does not access the system on Microsoft Windows through a suitable application programming interface but runs as a driver in ring 0 to have elevated privileges on the operating system. However, a crash in this area leads to a blue screen of death, which stops the operating system.  

On the user side, it is mentioned that the outage caused disruptions within critical infrastructures and that certain measures could have avoided or reduced the disruption time:
- Elaborating and testing the disaster recovery plan + incident response plan.
- Ensure backup are up to date and tested.
- Check whether disaster recovery have been conducted yearly.
- Give IT technicians Pizzas and donuts, they should resolve the issue faster.


**As IT security expert we want to ensure that generic change management processes lead to the creation of a sane genAI solution. How to do that?**  
-Check the quality of the Change Request Process, this should includes submitting, reviewing and approving change requests.  
-Verify whether deployment methodology exists and the impact for deploying solution in production is measured and detailed.  
-Check whether version controls exists.  
-Assess the implementation of the development and production pipelines. (Unit tests, integration tests and perf' evaluation)  
-Check how stakeholders are involved, to keep in sync with expectations.  
-Verify documentation exists.  
-Verify whether a monitoring and a feedback process are set up and how it supports future improvements.  
-Check whether the solution complies with relevant regulations, laws and acts. (ex: EU act)  
